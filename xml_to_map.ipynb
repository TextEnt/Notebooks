{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf5a538-f084-4cfd-849f-34d9115d3ebf",
   "metadata": {},
   "source": [
    "# NER to Wikidata for coordinate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c365f7-34c1-4078-a4fc-75a97928349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "import sparql_dataframe\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from shapely.wkt import loads\n",
    "import pydeck as pdk\n",
    "import shapely.wkt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d04c1e-d095-4864-8aef-93afac9cb445",
   "metadata": {},
   "source": [
    "## Extracting XML elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952de2b9-8b56-4cca-8a9c-a59dc4322ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elements(xml_file):\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "    with open(xml_file, 'rb') as file:\n",
    "        tree = etree.parse(file)\n",
    "    \n",
    "    elements = {\n",
    "        'placeName': tree.xpath('/tei:TEI/tei:text/tei:body/tei:sp/tei:ab/tei:seg/tei:reg/tei:placeName', namespaces=namespaces),\n",
    "        'persName': tree.xpath('/tei:TEI/tei:text/tei:body/tei:sp/tei:ab/tei:seg/tei:reg/tei:persName', namespaces=namespaces),\n",
    "        'target': tree.xpath('//tei:ptr/@target', namespaces=namespaces),\n",
    "        'author': tree.xpath('//tei:author/tei:persName/text()', namespaces=namespaces),\n",
    "        'title': tree.xpath('//tei:title/text()', namespaces=namespaces),\n",
    "        'pubPlace': tree.xpath('//tei:pubPlace/text()', namespaces=namespaces),\n",
    "        'date': tree.xpath('//tei:TEI/tei:teiHeader/tei:fileDesc/tei:sourceDesc/tei:bibl/tei:date/@when', namespaces=namespaces)\n",
    "    }\n",
    "    \n",
    "    return tree, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c4a32-f229-424e-a624-11f790a1de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_folder(folder_path):\n",
    "    data_place = []\n",
    "    data_pers = []\n",
    "    \n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing XML files\"):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            tree, elements = extract_elements(file_path)\n",
    "            \n",
    "            placeNames = [el.text for el in elements['placeName']]\n",
    "            persNames = [el.text for el in elements['persName']]\n",
    "            target = elements['target'][0] if elements['target'] else None\n",
    "            author = elements['author'][0] if elements['author'] else None\n",
    "            title = elements['title'][0] if elements['title'] else None\n",
    "            pubPlace = elements['pubPlace'][0] if elements['pubPlace'] else None\n",
    "            date = elements['date'][0] if elements['date'] else None\n",
    "            \n",
    "            placeName_counts = Counter(placeNames)\n",
    "            persName_counts = Counter(persNames)\n",
    "            \n",
    "            for placeName in set(placeNames):\n",
    "                data_place.append({\n",
    "                    'target': target,\n",
    "                    'title': title,\n",
    "                    'pubPlace': pubPlace,\n",
    "                    'date': date,\n",
    "                    'author': author,\n",
    "                    'place': placeName,\n",
    "                    'placeName_num': placeName_counts[placeName]\n",
    "                })\n",
    "                \n",
    "            for persName in set(persNames):\n",
    "                data_pers.append({\n",
    "                    'target': target,\n",
    "                    'title': title,\n",
    "                    'pubPlace': pubPlace,\n",
    "                    'date': date,\n",
    "                    'author': author,\n",
    "                    'person': persName,\n",
    "                    'persName_num': persName_counts[persName]\n",
    "                })\n",
    "    \n",
    "    return data_place, data_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd2885-cad5-40ad-9ebb-d7a91165e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(data_place, data_pers):\n",
    "    df_place = pd.DataFrame(data_place)\n",
    "    df_pers = pd.DataFrame(data_pers)\n",
    "    \n",
    "    df_place = df_place.drop_duplicates().sort_values(by='placeName_num', ascending=False)\n",
    "    df_pers = df_pers.drop_duplicates().sort_values(by='persName_num', ascending=False)\n",
    "    \n",
    "    return df_place, df_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da203221-25f7-4254-bbff-417260dfabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/nicola/Documents/Academia/Projects/TextEnt/Processing/NER'\n",
    "\n",
    "data_place, data_pers = process_xml_folder(folder_path)\n",
    "df_place, df_pers = create_dataframes(data_place, data_pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778d395-3c80-401f-9a03-fbd209af4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place[\"place\"] = df_place[\"place\"].str.lower()\n",
    "df_place.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaebb69-1a0e-48f5-8b3d-e78432918320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pers[\"person\"] = df_pers[\"person\"].str.lower()\n",
    "df_pers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c05a1c-d6cb-47ea-a323-a421a3850c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_filter = df_place.groupby('target', group_keys=False).apply(lambda x: x.nlargest(2, 'placeName_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801598fd-7640-4077-99e8-159254b7f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_filter['uuid'] = df_place_filter['place'].apply(lambda x: str(uuid.uuid5(uuid.NAMESPACE_DNS, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386010ee-cb84-430f-9b20-01cdc913e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196a45c-0e98-4ad9-9d39-86634978e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_place.to_csv('/Users/carboni/Documents/Academia/Projects/TextEnt/output/df_place.csv', index=False)\n",
    "#df_pers.to_csv('/Users/carboni/Documents/Academia/Projects/TextEnt/output/df_pers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd7c6f-c311-49d1-8694-533d4d84672b",
   "metadata": {},
   "source": [
    "### Removing Articles from NER results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51906a0-1a55-4c98-8073-9ce808dc1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    \"l'\",\n",
    "    \"le \",\n",
    "    \"la \",\n",
    "    \"les \",\n",
    "    \"un \",\n",
    "    \"une \",\n",
    "    \"des \",\n",
    "    \"du \",\n",
    "    \"de la \",\n",
    "    \"de l'\",\n",
    "    \"de \",\n",
    "    \"d'\",\n",
    "    \"au \",\n",
    "    \"aux \",\n",
    "    \"Ã  \",\n",
    "    \"chez \",\n",
    "    \"sur \",\n",
    "    \"en \",\n",
    "    \"dans \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86b21a-483a-453d-84af-61b0065ce4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = sorted(articles, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c015e-f7c8-4e76-9142-87a60d459991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_article(place):\n",
    "    # Check if the place starts with any of the articles\n",
    "    for article in articles:\n",
    "        if place.lower().startswith(article.lower()):  # Case-insensitive match\n",
    "            return place[len(article):].strip()  # Remove the article and strip leading spaces\n",
    "    return place  # Return the original if no article is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8727b7d-d4f9-42a1-8eae-301caf7dc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_filter['place'] = df_place_filter['place'].apply(remove_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d75f20-60b9-4477-bb79-7df068a1139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_place_filter[df_place_filter['uuid'].str.contains('9aebb788-d4a9-5959-b6ab-929f47c17c40')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31547eb9-13f4-4fcf-b4d4-7e456e890611",
   "metadata": {},
   "source": [
    "## Wikidata Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b614577-cc69-42ba-bd60-60a805f19e7f",
   "metadata": {},
   "source": [
    "### Mythological Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff789954-02f7-49ab-b531-8fbc74a83402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_filter['place'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b56fe6-c4d7-47d0-9e08-c2271c54b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sparql_for_mythological_places(\n",
    "    df,\n",
    "    place_column='place',\n",
    "    target_column='target',\n",
    "    uuid_column='uuid',\n",
    "    endpoint_url='http://10.194.68.72:7001/sparql',\n",
    "    output_columns=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Queries the SPARQL endpoint for each unique place in the DataFrame and returns the results as a new DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the places to query.\n",
    "        place_column (str): The name of the column containing the place names.\n",
    "        target_column (str): The name of the column containing the target values (to be included in the output).\n",
    "        uuid_column (str): The name of the column containing the UUIDs (to be included in the output).\n",
    "        endpoint_url (str): The URL of the SPARQL endpoint.\n",
    "        output_columns (list): List of additional output columns to include. If None, defaults to extracting the usual fields.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the query results.\n",
    "    \"\"\"\n",
    "    if output_columns is None:\n",
    "        output_columns = [\n",
    "            \"place\", \"target\", \"uuid\", \"wikidata_id\", \"name\", \"coordinates\",\n",
    "            \"typeLabel\", \"culture\", \"start_time\", \"end_time\", \"country\",\n",
    "            \"countryLabels\", \"osm_id\", \"sitelinks\", \"roman_atlas_id\",\n",
    "            \"pleiades_id\", \"topostext_id\", \"myths_id\", \"poleis_id\", \"manto_id\"\n",
    "        ]\n",
    "    \n",
    "    # SPARQL query template with placeholder for the place name\n",
    "    query_template = \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    SELECT ?a ?name ?coordinates ?typeLabel ?culture ?start_time ?end_time ?country (group_concat(?countryLabel; separator=\";; \") AS ?countryLabels) ?osm_id ?sitelinks ?roman_atlas_id ?pleiades_id ?topostext_id ?myths_id ?poleis_id ?manto_id   \n",
    "        WHERE {{\n",
    "          {{\n",
    "            SELECT ?a (MAX(?sitelinks) AS ?maxSitelinks)\n",
    "            WHERE {{\n",
    "              VALUES ?category {{ wd:Q3238337 }} # only mythical location    \n",
    "              ?a (wdt:P31)/((wdt:P279)*) ?category.\n",
    "              ?a rdfs:label ?name .\n",
    "              ?a ^schema:about/wikibase:sitelinks ?sitelinks .\n",
    "\n",
    "              FILTER (LANG(?name) = \"fr\" || LANG(?name) = \"en\") .\n",
    "              FILTER REGEX(STR(?name), \"{value}\", \"i\") .\n",
    "            }}\n",
    "            GROUP BY ?a ?name\n",
    "            ORDER BY DESC(?maxSitelinks)\n",
    "            LIMIT 1\n",
    "          }}\n",
    "\n",
    "          ?a rdfs:label ?name .\n",
    "          ?a wdt:P31 ?type .\n",
    "          ?a wdt:P625 ?coordinates . \n",
    "          ?a ^schema:about/wikibase:sitelinks ?sitelinks .\n",
    "\n",
    "          ?type rdfs:label ?typeLabel .\n",
    "\n",
    "          OPTIONAL {{\n",
    "            ?a wdt:P361 ?partOf .\n",
    "            ?partOf rdfs:label ?partOfLabel .\n",
    "            FILTER (LANG(?partOfLabel) = \"en\") .\n",
    "          }}\n",
    "\n",
    "          OPTIONAL {{\n",
    "            ?a wdt:P17 ?country .\n",
    "            ?country rdfs:label ?countryLabel .\n",
    "            FILTER (LANG(?countryLabel) = \"en\") .\n",
    "          }}\n",
    "\n",
    "        OPTIONAL {{\n",
    "             ?a wdt:P2596 ?culture . \n",
    "             ?culture wdt:P580 ?start_time .\n",
    "             ?culture wdt:P582 ?end_time .\n",
    "          }} \n",
    "\n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P1584 ?pleiades_id\n",
    "          }}\n",
    "\n",
    "         OPTIONAL {{\n",
    "            ?a wdt:P402 ?osm_id\n",
    "          }}\n",
    "\n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P8068 ?topostext_id . \n",
    "          }}\n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P361 ?partOf\n",
    "          }}\n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P1936 ?roman_atlas_id . \n",
    "          }}  \n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P12402 ?myths_id . \n",
    "          }}  \n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P8137 ?poleis_id .\n",
    "          }}\n",
    "        OPTIONAL {{\n",
    "            ?a wdt:P9736 ?manto_id .\n",
    "          }} \n",
    "\n",
    "          FILTER (LANG(?name) = \"en\") .\n",
    "          FILTER (LANG(?typeLabel) = \"en\") .\n",
    "        }}\n",
    "        GROUP BY ?a ?name ?coordinates ?typeLabel ?culture ?start_time ?end_time ?country ?sitelinks ?osm_id ?roman_atlas_id ?pleiades_id ?topostext_id ?myths_id ?poleis_id ?manto_id\n",
    "        ORDER BY DESC(?sitelinks)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Deduplicate the places to avoid querying the same place twice\n",
    "    unique_places = df[[place_column, target_column, uuid_column]].drop_duplicates(subset=[place_column])\n",
    "    \n",
    "    # Iterate over the DataFrame and query for each unique place\n",
    "    for _, row in tqdm(unique_places.iterrows(), total=len(unique_places), desc=\"Querying SPARQL Endpoint\"):\n",
    "        place_name = row[place_column]\n",
    "        target = row[target_column]\n",
    "        uuid = row[uuid_column]\n",
    "\n",
    "        # Replace {value} with the place name\n",
    "        query = query_template.format(value=place_name)\n",
    "\n",
    "        try:\n",
    "            # Send the request to the SPARQL endpoint\n",
    "            response = requests.get(endpoint_url, params={'query': query, 'format': 'json'})\n",
    "            if response.status_code == 200:\n",
    "                query_result = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "                \n",
    "                if query_result:\n",
    "                    for result in query_result:\n",
    "                        # Extract data from the JSON response\n",
    "                        place_data = {\n",
    "                            \"place\": place_name,\n",
    "                            \"target\": target,\n",
    "                            \"uuid\": uuid,\n",
    "                            \"wikidata_id\": result.get(\"a\", {}).get(\"value\", \"\"),\n",
    "                            \"name\": result.get(\"name\", {}).get(\"value\", \"\"),\n",
    "                            \"coordinates\": result.get(\"coordinates\", {}).get(\"value\", \"\"),\n",
    "                            \"typeLabel\": result.get(\"typeLabel\", {}).get(\"value\", \"\"),\n",
    "                            \"culture\": result.get(\"culture\", {}).get(\"value\", \"\"),\n",
    "                            \"start_time\": result.get(\"start_time\", {}).get(\"value\", \"\"),\n",
    "                            \"end_time\": result.get(\"end_time\", {}).get(\"value\", \"\"),\n",
    "                            \"country\": result.get(\"country\", {}).get(\"value\", \"\"),\n",
    "                            \"countryLabels\": result.get(\"countryLabels\", {}).get(\"value\", \"\"),\n",
    "                            \"osm_id\": result.get(\"osm_id\", {}).get(\"value\", \"\"),\n",
    "                            \"sitelinks\": result.get(\"sitelinks\", {}).get(\"value\", \"\"),\n",
    "                            \"roman_atlas_id\": result.get(\"roman_atlas_id\", {}).get(\"value\", \"\"),\n",
    "                            \"pleiades_id\": result.get(\"pleiades_id\", {}).get(\"value\", \"\"),\n",
    "                            \"topostext_id\": result.get(\"topostext_id\", {}).get(\"value\", \"\"),\n",
    "                            \"myths_id\": result.get(\"myths_id\", {}).get(\"value\", \"\"),\n",
    "                            \"poleis_id\": result.get(\"poleis_id\", {}).get(\"value\", \"\"),\n",
    "                            \"manto_id\": result.get(\"manto_id\", {}).get(\"value\", \"\")\n",
    "                        }\n",
    "                        results.append(place_data)\n",
    "                else:\n",
    "                    print(f\"No data found for place: {place_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for place: {place_name}, Status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error querying API for place: {place_name}, Error: {e}\")\n",
    "\n",
    "        # Wait for 1 second to avoid overloading the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Convert the list of results into a DataFrame\n",
    "    place_mythological_only = pd.DataFrame(results, columns=output_columns)\n",
    "    \n",
    "    return place_mythological_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc52d0-ca3b-4893-9043-03b64383216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_mythological_only = query_sparql_for_mythological_places(df_place_filter, place_column='place', target_column='target', uuid_column='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e4593-1942-4f6f-9960-a67a202b7708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c662d441-6acc-4d82-8c0f-88a02e16e528",
   "metadata": {},
   "source": [
    "### Historical Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d95993-e93d-44fc-92d1-df23c1936a1d",
   "metadata": {},
   "source": [
    "### All other Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29c137-fd60-491f-a875-22896364b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(results, filename='sparql_results_temp.csv'):\n",
    "    temp_df = pd.DataFrame(results)\n",
    "    temp_df.to_csv(filename, index=False)\n",
    "\n",
    "def load_progress(filename='sparql_results_temp.csv'):\n",
    "    if os.path.exists(filename):\n",
    "        temp_df = pd.read_csv(filename)\n",
    "        return temp_df.to_dict('records')\n",
    "    return []\n",
    "\n",
    "def load_query_cache(filename='query_cache.pkl'):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_query_cache(cache, filename='query_cache.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49569e-708c-4a78-a0c8-61b769c14447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_wikidata(place):\n",
    "    if place in query_cache:\n",
    "        return query_cache[place]\n",
    "\n",
    "    query_template = \"\"\"\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "\n",
    "    SELECT ?a ?name ?coordinates ?typeLabel ?culture ?start_time ?end_time ?country (group_concat(?countryLabel; separator=\";; \") AS ?countryLabels) ?osm_id ?sitelinks ?roman_atlas_id ?pleiades_id ?topostext_id ?myths_id ?poleis_id ?manto_id   \n",
    "    WHERE {{\n",
    "      {{\n",
    "        SELECT ?a (MAX(?sitelinks) AS ?maxSitelinks)\n",
    "        WHERE {{\n",
    "          VALUES ?category {{ wd:Q6256 wd:Q82794 wd:Q1620908 }}\n",
    "          ?a (wdt:P31)/((wdt:P279)*) ?category.\n",
    "          ?a rdfs:label ?name .\n",
    "          ?a ^schema:about/wikibase:sitelinks ?sitelinks .\n",
    "\n",
    "          FILTER (LANG(?name) = \"fr\") .\n",
    "          FILTER REGEX(STR(?name), \"^{place}$\", \"i\") .\n",
    "        }}\n",
    "        GROUP BY ?a\n",
    "        ORDER BY DESC(?maxSitelinks)\n",
    "        LIMIT 1\n",
    "      }}\n",
    "\n",
    "      ?a rdfs:label ?name .\n",
    "      ?a wdt:P31 ?type .\n",
    "      ?a wdt:P625 ?coordinates . \n",
    "      ?a ^schema:about/wikibase:sitelinks ?sitelinks .\n",
    "     \n",
    "      ?type rdfs:label ?typeLabel .\n",
    "      \n",
    "      \n",
    "      OPTIONAL {{\n",
    "        ?a wdt:P17 ?country .\n",
    "        ?country rdfs:label ?countryLabel .\n",
    "        FILTER (LANG(?countryLabel) = \"en\") .\n",
    "      }}\n",
    "      \n",
    "    OPTIONAL {{\n",
    "         ?a wdt:P2596 ?culture . \n",
    "         ?culture wdt:P580 ?start_time .\n",
    "         ?culture wdt:P582 ?end_time .\n",
    "      }} \n",
    "      \n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P1584 ?pleiades_id\n",
    "      }}\n",
    "\n",
    "     OPTIONAL {{\n",
    "        ?a wdt:P402 ?osm_id\n",
    "      }}\n",
    "      \n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P8068 ?topostext_id . \n",
    "      }}\n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P361 ?partOf\n",
    "      }}\n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P1936 ?roman_atlas_id . \n",
    "      }}  \n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P12402 ?myths_id . \n",
    "      }}  \n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P8137 ?poleis_id .\n",
    "      }}\n",
    "    OPTIONAL {{\n",
    "        ?a wdt:P9736 ?manto_id .\n",
    "      }} \n",
    "\n",
    "      FILTER (LANG(?name) = \"en\") .\n",
    "      FILTER (LANG(?typeLabel) = \"en\") .\n",
    "    }}\n",
    "    GROUP BY ?a ?name ?coordinates ?typeLabel ?culture ?start_time ?end_time ?country ?sitelinks ?osm_id ?roman_atlas_id ?pleiades_id ?topostext_id ?myths_id ?poleis_id ?manto_id\n",
    "    ORDER BY DESC(?sitelinks)\n",
    "    \"\"\"\n",
    "    \n",
    "    query = query_template.format(place=place)\n",
    "    #url = 'https://qlever.cs.uni-freiburg.de/api/wikidata'\n",
    "    url = 'http://10.194.68.72:7001' #internal unige\n",
    "    response = requests.get(url, params={'query': query, 'output': 'json'})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        raw_results = response.json().get('results', {}).get('bindings', [])\n",
    "        # Process results to ensure all values are strings\n",
    "        processed_results = []\n",
    "        for result in raw_results:\n",
    "            processed_result = {key: str(value.get('value', '')) for key, value in result.items()}\n",
    "            processed_results.append(processed_result)\n",
    "        query_cache[place] = processed_results\n",
    "        return processed_results\n",
    "    else:\n",
    "        query_cache[place] = None  # Cache failed queries as well\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641c9cf-ac78-41cb-9657-7a22d6d58baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize or load query cache\n",
    "query_cache = load_query_cache()\n",
    "\n",
    "# Load existing results if any\n",
    "results = load_progress()\n",
    "\n",
    "# Get the set of already processed places\n",
    "processed_places = set([entry['place'] for entry in results])\n",
    "\n",
    "# Extract unique places from your DataFrame\n",
    "unique_places_df = df_place_filter.drop_duplicates(subset='place')\n",
    "\n",
    "# Initialize a counter for saving progress periodically\n",
    "counter = 0\n",
    "save_every_n = 10  # Save progress every N iterations\n",
    "\n",
    "# Iterate over unique places\n",
    "for index, row in tqdm(unique_places_df.iterrows(), total=len(unique_places_df), desc=\"Querying QLever\"):\n",
    "    place = row['place']\n",
    "    target = row['target']\n",
    "    uuid = row['uuid']\n",
    "    \n",
    "    if place in processed_places:\n",
    "        continue\n",
    "    \n",
    "    sparql_results = query_wikidata(place)\n",
    "    \n",
    "    if sparql_results:\n",
    "        for result in sparql_results:\n",
    "            results.append({\n",
    "                'place': place,\n",
    "                'target': target,\n",
    "                'uuid': uuid,\n",
    "                'wikidata_id': result.get('a', ''),\n",
    "                'name': result.get('name', ''),\n",
    "                'coordinates': result.get('coordinates', ''),\n",
    "                'typeLabel': result.get('typeLabel', ''),\n",
    "                'country': result.get('country', ''),\n",
    "                'countryLabel': result.get('countryLabels', ''),\n",
    "                'culture': result.get('culture', ''),\n",
    "                'start_time': result.get('start_time', ''),\n",
    "                'end_time': result.get('end_time', ''),\n",
    "                'partOf': result.get('partOf', ''),\n",
    "                'sitelinks': result.get('sitelinks', ''),\n",
    "                'osm_id': result.get('osm_id', ''),\n",
    "                'roman_atlas_id': result.get('roman_atlas_id', ''),\n",
    "                'pleiades_id': result.get('pleiades_id', ''),\n",
    "                'topostext_id': result.get('topostext_id', ''),\n",
    "                'myths_id': result.get('myths_id', ''),\n",
    "                'poleis_id': result.get('poleis_id', ''),\n",
    "                'manto_id': result.get('manto_id', '')\n",
    "            })\n",
    "    else:\n",
    "        # Handle cases where there is no result\n",
    "        results.append({\n",
    "            'place': place,\n",
    "            'uuid': uuid,\n",
    "            'target': target,\n",
    "            'wikidata_id': '',\n",
    "            'name': '',\n",
    "            'coordinates': '',\n",
    "            'typeLabel': '',\n",
    "            'country': '',\n",
    "            'countryLabels': '',\n",
    "            'culture': '',\n",
    "            'start_time': '',\n",
    "            'end_time': '',\n",
    "            'partOf': '',\n",
    "            'sitelinks': '',\n",
    "            'osm_id': '',\n",
    "            'roman_atlas_id': '',\n",
    "            'pleiades_id': '',\n",
    "            'topostext_id': '',\n",
    "            'myths_id': '',\n",
    "            'poleis_id': '',\n",
    "            'manto_id': ''\n",
    "        })\n",
    "    \n",
    "    processed_places.add(place)\n",
    "    counter += 1\n",
    "    \n",
    "    # Save progress and cache every N iterations\n",
    "    if counter % save_every_n == 0:\n",
    "        save_progress(results)\n",
    "        save_query_cache(query_cache)\n",
    "        # Optionally, print a message\n",
    "        # print(f\"Saved progress after processing {counter} places.\")\n",
    "    \n",
    "    # Optional: Remove or adjust sleep time if necessary\n",
    "    time.sleep(1)\n",
    "\n",
    "# After processing all places, save final progress and cache\n",
    "save_progress(results)\n",
    "save_query_cache(query_cache)\n",
    "\n",
    "# Convert the results to a new DataFrame\n",
    "df_place_coordinates = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c7d0b-c968-4ee9-b88b-79e4c01cb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm query_cache.pkl\n",
    "#!rm sparql_results_temp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf5ec94-bb82-4054-aa45-a31dcc530f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_place_coordinates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_place_coordinates\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_place_coordinates' is not defined"
     ]
    }
   ],
   "source": [
    "df_place_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce51da-b116-45da-a0ce-dc9ee5faa976",
   "metadata": {},
   "source": [
    "### Concat classes and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eec7b4-f9ea-47ae-842c-356b239e72db",
   "metadata": {},
   "source": [
    "aggregate instead of using SPARQL concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e9fe9-cd08-414c-9195-eb049b631bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_cols = ['typeLabel', 'countryLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9337a4e-9f89-42ac-8b71-fa4b86ef95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_func(col):\n",
    "    if col in concat_cols:\n",
    "        return lambda x: ';;'.join(map(str, x.unique()))\n",
    "    else:\n",
    "        return 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28891dc-4e12-4f16-9aba-cfef197cb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {col: get_agg_func(col) for col in df_place_coordinates.columns if col != 'uuid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a546e-455e-4328-912d-2f690f77f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat = df_place_coordinates.groupby('uuid', as_index=False).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993829ad-9ee5-4ca1-9f72-de5ced463fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c18b5-e504-4ecb-99c9-033c949b351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_place_concat.to_csv('df_place_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab90fa2-dc3a-44c2-a6ea-c45e389a31cc",
   "metadata": {},
   "source": [
    "## Query OSM for WKT Multipolygon (filtering labelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab7277-a600-4144-b2b5-6c44584447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_types = ['river', 'lake', 'region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b6a6f-1392-45dc-a9f6-3f12a95a5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_place_concat['typeLabel'].str.contains('|'.join(filter_types), case=False, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b6cbf-d74b-411c-bf60-5591203d0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask & df_place_concat['osm_id'].notnull()\n",
    "df_filtered = df_place_concat[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86436089-9ab8-49fa-8a4d-a9ea6853dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_osm_wkt(osm_id):\n",
    "    # Ensure osm_id is a string (in case it's not)\n",
    "    osm_id_str = str(osm_id).strip()\n",
    "    \n",
    "    # Build the SPARQL query, substituting the osm_id\n",
    "    query_template = \"\"\"\n",
    "    PREFIX ogc: <http://www.opengis.net/rdf#>\n",
    "    PREFIX osmrel: <https://www.openstreetmap.org/relation/>\n",
    "    PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "    PREFIX osm: <https://www.openstreetmap.org/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX osmkey: <https://www.openstreetmap.org/wiki/Key:>\n",
    "    SELECT ?shape WHERE {{\n",
    "      osmrel:{osm_id} geo:hasGeometry/geo:asWKT ?shape\n",
    "    }}\n",
    "    \"\"\"\n",
    "    query = query_template.format(osm_id=osm_id_str)\n",
    "    url = 'https://qlever.cs.uni-freiburg.de/api/osm-planet'\n",
    "\n",
    "    # Send the request\n",
    "    try:\n",
    "        response = requests.get(url, params={'query': query, 'output': 'json'})\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('results', {}).get('bindings', [])\n",
    "            if results:\n",
    "                # Get the 'shape' value\n",
    "                shape = results[0].get('shape', {}).get('value', '')\n",
    "                return shape\n",
    "            else:\n",
    "                # No results found\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error querying osm_id {osm_id}: HTTP {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception querying osm_id {osm_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c8ac5-f64c-453a-8012-cc6d1ff69241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_osm_wkt(row):\n",
    "    return get_osm_wkt(row['osm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a9567-ab98-443a-b4b5-d97118676963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['osm_wkt'] = df_filtered.apply(fetch_osm_wkt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a68cb-194d-493c-ad7a-ec6be809d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0f019-9ecf-434c-a096-3f69aae0130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat['osm_wkt'] = None\n",
    "df_place_concat.loc[df_filtered.index, 'osm_wkt'] = df_filtered['osm_wkt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ecd5e-064e-4eec-a57b-4216301a8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat.to_csv('df_place_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b34fc-046f-4d8f-89d4-dba7f6f7fae3",
   "metadata": {},
   "source": [
    "## Query WHG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888feac-acd7-46f3-b45c-9f5e665aeea3",
   "metadata": {},
   "source": [
    "#### Further Refiniment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bd59e-118d-4657-9967-422f0a8c7f41",
   "metadata": {},
   "source": [
    "if it is a river or a waterway, get the coordinate from OpenStreetMap. Possible to do using the instance in QLever (sparql = https://qlever.cs.uni-freiburg.de/api/osm-planet\n",
    ")\n",
    "```\n",
    "PREFIX ogc: <http://www.opengis.net/rdf#>\n",
    "PREFIX osmrel: <https://www.openstreetmap.org/relation/>\n",
    "PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "PREFIX osm: <https://www.openstreetmap.org/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX osmkey: <https://www.openstreetmap.org/wiki/Key:>\n",
    "SELECT * WHERE {\n",
    "  osmrel:2188548 geo:hasGeometry/geo:asWKT ?shape\n",
    "}\n",
    "```\n",
    "problems: somehow throgh python it does not retrieve the type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008d2b0-a08a-4fb9-bdba-e4e4f1e8b007",
   "metadata": {},
   "source": [
    "if results are empty, query TGN. Althought it is quite slow:\n",
    "\n",
    "```\n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?place ?geojson ?typeLabel WHERE {\n",
    "  ?place a crm:E53_Place .\n",
    "  ?place rdfs:label \"oristano\" .\n",
    "  ?place crm:P1_is_identified_by ?geometry .\n",
    "  ?geometry crm:P2_has_type <http://geojson.org> ;\n",
    "            crm:P90_has_value ?geojson .\n",
    "  ?place crm:P2_has_type ?type .\n",
    "  ?type rdfs:label ?typeLabel .\n",
    "} LIMIT 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38446a0e-ff3a-408e-b1ea-50415466d9e1",
   "metadata": {},
   "source": [
    "Other option, use world historical gazetteer API\n",
    "https://whgazetteer.org/api/index/?name=oristano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e726c-ea82-4fbc-9ed5-ed5657e09b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3bde7-a5c5-494a-8b1f-5f3c183c015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_whg_api_for_empty_coordinates(df, place_column='place', coordinates_column='coordinates', log_file='result_whg.txt'):\n",
    "    \"\"\"\n",
    "    Queries the WHG API for places where the coordinates are missing, retrieves longitude and latitude,\n",
    "    and stores them as a POINT string directly in the existing coordinates column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        place_column (str): The name of the column containing the place names to query.\n",
    "        coordinates_column (str): The name of the column where coordinates are missing and to store the POINT string.\n",
    "        log_file (str): The file where messages for places with no features found will be logged.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the updated coordinates column containing the POINT string.\n",
    "    \"\"\"\n",
    "    # Check which rows have empty 'coordinates'\n",
    "    empty_coordinates_mask = df[coordinates_column].isna() | (df[coordinates_column] == '')\n",
    "    \n",
    "    # Get rows with empty 'coordinates' for iteration\n",
    "    rows_to_query = df[empty_coordinates_mask]\n",
    "    \n",
    "    # Open the log file to write messages\n",
    "    with open(log_file, 'a') as log:\n",
    "        # Iterate over rows with empty 'coordinates' using tqdm for progress bar\n",
    "        for index, row in tqdm(rows_to_query.iterrows(), total=len(rows_to_query), desc=\"Querying WHG API\"):\n",
    "            place_name = row[place_column]\n",
    "\n",
    "            # Construct the API URL\n",
    "            api_url = f\"https://whgazetteer.org/api/index/?name={place_name}\"\n",
    "\n",
    "            try:\n",
    "                # Make the API request\n",
    "                response = requests.get(api_url)\n",
    "\n",
    "                # Check if the request was successful (status code 200)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    # Extract the coordinates using the JSON path\n",
    "                    if 'features' in data and len(data['features']) > 0:\n",
    "                        geometry = data['features'][0].get('geometry', None)\n",
    "                        if geometry and 'coordinates' in geometry:\n",
    "                            coordinates = geometry['coordinates']\n",
    "                            if len(coordinates) >= 2:\n",
    "                                longitude = coordinates[0]\n",
    "                                latitude = coordinates[1]\n",
    "                                \n",
    "                                # Create the POINT string\n",
    "                                point_str = f\"POINT({longitude} {latitude})\"\n",
    "                                df.at[index, coordinates_column] = point_str\n",
    "                            else:\n",
    "                                log.write(f\"Coordinates not found for place: {place_name}\\n\")\n",
    "                                df.at[index, coordinates_column] = None\n",
    "                        else:\n",
    "                            log.write(f\"No valid geometry found for place: {place_name}\\n\")\n",
    "                            df.at[index, coordinates_column] = None\n",
    "                    else:\n",
    "                        log.write(f\"No features found for place: {place_name}\\n\")\n",
    "                        df.at[index, coordinates_column] = None\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data for place: {place_name}, Status code: {response.status_code}\")\n",
    "                    df.at[index, coordinates_column] = None  # Save None if the request failed\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error querying API for place: {place_name}, Error: {e}\")\n",
    "                df.at[index, coordinates_column] = None  # Save None if there was an error\n",
    "\n",
    "            # Wait for 2 seconds before the next request\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9aa58-f07e-44d4-a814-0220b50304a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat = query_whg_api_for_empty_coordinates(df_place_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a7622-bf1f-44a5-8bbb-260cdbb73bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47a1b8-792c-478c-856b-ed965afb5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86668670-5c74-46ed-9de3-cdc6048f0da5",
   "metadata": {},
   "source": [
    "## Map Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543d14e-8b3a-470f-ab26-686bb3a0fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat = df_place_concat.dropna(subset=['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa72076-9534-4bea-9e9c-8e5d198b35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat['place_number'] = df_place_concat.groupby(['place', 'country'])['place'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd44d8-49c4-4020-92b0-c0f7d8a97515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat = df_place_concat[df_place_concat['coordinates'] != \"bn860104\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1daf8a-33d2-46b8-bea1-62374a90dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat['geometry'] = df_place_concat['coordinates'].apply(loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335b677-6569-41df-81ea-1378e0e78d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_concat['latitude'] = df_place_concat['geometry'].apply(lambda geom: geom.y)\n",
    "df_place_concat['longitude'] = df_place_concat['geometry'].apply(lambda geom: geom.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cc867-e39c-48f3-a239-68e838f2aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_place_concat.drop_duplicates(subset=['place', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7c54b-02a2-4dff-af7b-5b668c9b8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b490c-e703-4f69-8d2f-37092575dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_no_duplicates.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a626e45-9465-49d3-a690-af232618f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_layer = pdk.Layer(\n",
    "    'ScatterplotLayer',\n",
    "    df_no_duplicates,\n",
    "    opacity=0.6,\n",
    "    get_position=['longitude', 'latitude'],\n",
    "    get_radius='place_number * 5000',\n",
    "    get_fill_color=[255, 0, 0],  # Red \n",
    "    pickable=True,\n",
    "    stroked=True,\n",
    "    get_line_color=[255,255,255]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fdf4a-db61-46f8-b5d8-54cf4dfdce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_state = pdk.ViewState(\n",
    "    latitude=df_no_duplicates['latitude'].mean(),\n",
    "    longitude=df_no_duplicates['longitude'].mean(),\n",
    "    zoom=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a46eaf-9b30-4bce-b514-8b8792655072",
   "metadata": {},
   "outputs": [],
   "source": [
    "tooltip = {\n",
    "    \"html\": \"<b>{place_number}</b> reference to <b>{place}</b>\",\n",
    "    \"style\": {\"background\": \"grey\", \"color\": \"white\", \"font-family\": '\"Helvetica Neue\", Arial', \"z-index\": \"10000\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999ae45-36d3-40a3-bf10-f0dbd0a9fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = pdk.Deck(\n",
    "    layers=[scatter_layer],\n",
    "    initial_view_state=view_state,\n",
    "    tooltip=tooltip,\n",
    "    map_provider=\"carto\",\n",
    "    map_style=\"light\" #possible here to go for lightâ, âdarkâ, âroadâ, âsatelliteâ, \n",
    "    #âdark_no_labelsâ, and âlight_no_labelsâ. Also possible to use mapbox. To change together with the \n",
    "    #parameters on scatter_layer (e.g. opacity!)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21bc52-9743-41ac-b355-a8b72b8b41b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck.to_html(filename='textent_map.html', offline=True, open_browser=False, notebook_display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda2480-bfd5-414a-8b42-75e3a6772cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fb535-1ec2-46a9-a042-68a28685caa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32236ace-72c3-422a-ba51-afd451315d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf6590-b24f-4888-8a97-1f5602e9a85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
