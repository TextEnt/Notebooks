{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "sys.path.append(\"../\")\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from typing import List, Tuple\n",
    "from lib.utils import tei2spacy, nlp_model_fr, sample_files, print_corpus_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_CORPUS_SERIALIZED_PATH = \"./data/corpus.spacy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded serialize spacy corpus from ./data/corpus.spacy\n",
      "Number of documents in the corpus: 12\n",
      "Number of entities in the corpus: 3204\n",
      "Number of tokens in the corpus: 184314\n"
     ]
    }
   ],
   "source": [
    "if Path(SPACY_CORPUS_SERIALIZED_PATH).exists():\n",
    "    spacy_corpus = DocBin(store_user_data=True).from_disk(SPACY_CORPUS_SERIALIZED_PATH)\n",
    "    print(f\"Loaded serialize spacy corpus from {SPACY_CORPUS_SERIALIZED_PATH}\")\n",
    "    print_corpus_summary(spacy_corpus, nlp_model_fr)\n",
    "else:\n",
    "    spacy_corpus = DocBin(store_user_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = spacy_corpus.get_docs(nlp_model_fr.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding entity linking information to docs in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_fishing_pipe = nlp_model_fr.add_pipe(\n",
    "    \"entityfishing\", config={\n",
    "        \"api_ef_base\": \"http://nerd.huma-num.fr/nerd/service\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_model_fr.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_docs = [\n",
    "    entity_fishing_pipe(spacy_doc)\n",
    "    for spacy_doc in docs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of salient sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add documentation and move to lib/utils.py\n",
    "\n",
    "import operator\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    qid: str\n",
    "    ner_labels: List[str] # ner tags for entity mentions\n",
    "    mention_frequency: int # document-level mention frequency\n",
    "    unique_surface_forms: List[str]\n",
    "    short_desc: str\n",
    "\n",
    "class SalientSentenceSelector(object):\n",
    "    \n",
    "    def __init__(self, spacy_doc: Doc):\n",
    "        self.doc = spacy_doc\n",
    "        self.entities = self._mentions2entities()\n",
    "        self.sentences = {sent_i + 1: sent for sent_i, sent in enumerate(self.doc.sents)}\n",
    "        self.sent2ent_idx = self._build_sentence2entity_index()\n",
    "\n",
    "    def _mentions2entities(self) -> List[str]:\n",
    "        # transform the entity mentions from spacy into a dataframe for easier manipulation\n",
    "        self._mentions_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    'mention': ent.text,\n",
    "                    'ner_label': ent.label_,\n",
    "                    'qid': ent._.kb_qid,\n",
    "                    'url_wikidata': ent._.url_wikidata,\n",
    "                    'nerd_score': ent._.nerd_score\n",
    "                }\n",
    "                for ent in self.doc.ents\n",
    "            ]\n",
    "        )\n",
    "        linked_entities_df = self._mentions_df[self._mentions_df.qid.notna()]\n",
    "        n_nonlinked_entities = len(self._mentions_df[self._mentions_df.qid.isna()])\n",
    "        n_linked_entities = len(linked_entities_df)\n",
    "        print(\n",
    "            f'Document {self.doc.user_data[\"filename\"]} contains {self._mentions_df.shape[0]} entities;',\n",
    "            f'{n_linked_entities} linked and {n_nonlinked_entities} non-linked'\n",
    "        )\n",
    "\n",
    "        # unique entities\n",
    "        unique_qids = linked_entities_df.qid.unique()\n",
    "        print(f'Document {self.doc.user_data[\"filename\"]} contains {len(unique_qids)} unique entities')\n",
    "\n",
    "        entities = []\n",
    "        for qid in unique_qids:\n",
    "            mentions  = linked_entities_df[linked_entities_df.qid == qid].mention\n",
    "            mention_frequency = len(mentions.tolist())\n",
    "            ner_labels = linked_entities_df[linked_entities_df.qid == qid].ner_label.unique().tolist()\n",
    "            unique_surface_forms = mentions.unique().tolist()\n",
    "            entities.append(\n",
    "                Entity(\n",
    "                    qid=qid,\n",
    "                    ner_labels=ner_labels,\n",
    "                    mention_frequency=mention_frequency,\n",
    "                    unique_surface_forms=unique_surface_forms,\n",
    "                    short_desc=''\n",
    "                )\n",
    "            )\n",
    "        # top_person\n",
    "        # top_place\n",
    "        return {entity.qid: entity for entity in entities}\n",
    "\n",
    "    def _build_sentence2entity_index(self) -> dict:\n",
    "        sentence2entity_index = {}\n",
    "        for sent_i, sent in self.sentences.items():\n",
    "            for ent in sent.ents:\n",
    "                if ent._.kb_qid:\n",
    "                    if sent_i not in sentence2entity_index:\n",
    "                        sentence2entity_index[sent_i] = set()\n",
    "                    sentence2entity_index[sent_i].add(ent._.kb_qid)\n",
    "        return sentence2entity_index\n",
    "\n",
    "    # TODO: select sentences for people and places separately\n",
    "    def _find_sentences_for_entity(self, entity: Entity) -> List[str]:\n",
    "        sentences = []\n",
    "        for sentence_id, entity_qids in self.sent2ent_idx.items():\n",
    "            if entity.qid in entity_qids:\n",
    "                sentences.append(self.sentences[sentence_id])\n",
    "        sentences.sort(key=lambda x: len(x), reverse=True)\n",
    "        return sentences\n",
    "\n",
    "    # very simplistic first implementation: take the most frequent entity (no diff. between people and places)\n",
    "    # and return the first `k`` sentences where the entity appears, ranked by sentence length (rationale: the longer, the more informative)\n",
    "    def select(self, top_k_sentences: int = 5) -> Tuple[Entity, List[str]]:\n",
    "        sorted_entities = sorted(self.entities.values(), key=operator.attrgetter('mention_frequency'), reverse=True)\n",
    "        top_entity = sorted_entities[0]\n",
    "        return (top_entity, self._find_sentences_for_entity(top_entity)[:top_k_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a sample document    \n",
    "sample_doc = el_docs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document bpt6k5458862p.tar.gz.tei_segmented_ner.xml contains 471 entities; 143 linked and 328 non-linked\n",
      "Document bpt6k5458862p.tar.gz.tei_segmented_ner.xml contains 56 unique entities\n"
     ]
    }
   ],
   "source": [
    "sss = SalientSentenceSelector(sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Entity(qid='Q18190448', ner_labels=['PER'], mention_frequency=52, unique_surface_forms=['Guillaume'], short_desc=''),\n",
       " [Guillaume Que c'est un grapd trésor que notre liberté, Qu'on la compare mal au bien de la clartète, Que l'usage en est doux, et qu'au temps où nous sommes, Un bien qui vient du Ciel se vend mal à des hommes, Le libéral arbitre est un don précieux Par où nous éprouvons la clémence des Dieux, Un esprit franc et libre où la raison abonde Enne possédant rien, possède tout le monde, Il ne dépend jamais de tant d'esprits divers, Au lieu d'une maison il a tour d'Unissers, Sa richesse le suit, et l'âme, sans contrainte, Sans auoir ai vertu sans con visage empreinte, LES PÂTONS,\n",
       "  Quand elle aura connu mon âme par ta bouche, Regarde dans son teint comment cela la touche, Prends garde si l'amour retient ensevelis Sous un peu de rougeur, ses roses et ses lis, Compte tous ses soupirs, et te donne la gloire D'âpporter mon salut dans ta belle mémoire, Guillaume Tout ce que l'artifice aura d'inventions, Ce qu'on ne fît jamais aux autres passions, Les mouvements des sens, de l'âme, et du visage, Tous les vices d'un fol, et les vertus d'un sage, Pont et quon nonmne peur, et te qnitne peur pas;,\n",
       "  Guillaume Ah, mon ami, qu'aimer est une belle chose, Si vous saviez l'état où votre vis maintenant, Je ne fais plus de vœux à Carême-prenanf, Bacchus m'est ennemi, je fais geler les treilles, J'ai hanni les festins, j'ai rompu les bouteilles, Je ne vis que d'amour, de cipre, et d'ambregris, Je pense que le Ciel est tombé dans Paris, Ne parlez plus de vous tant de vos Artenices, On n'a jamais rien venger d'égal à mes supplices, Comme elle vous trahit en quittant ce seiour.,\n",
       "  Amour vous fait mou Roi Guillaume faisant semblant d'allumer les feux de joie, embrase subtilement la ville par la persuation secrète d'Artenice, Je célèbre l'amour avec un feu de joie Dont on parlera plus que de celui de Troie, Dummierej cœur, le fuzil est ion, Et le monde va voir qu'on se chauffe la Cour.,\n",
       "  Mois -tu n'ignore pas qu'Amour a davantage 1 Que tour ce que les Dieux ont pris pour leur partage, Et qu en donnant moname à ton gouvernement C'est ce que tu demande à mon consentement: M'aime tu comme il saut, Guillaume Comme ici fais ma femme.])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the  SalientSentenceSelector to extract the top 5 sentences for the most frequent entity in the document\n",
    "# this entity context made of the k sentences can then be fed to an LLM (vel sim.) to characterise the spation-temporal\n",
    "# dimension of the document\n",
    "sss.select(top_k_sentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mention\n",
       "Calyante     70\n",
       "Guillaume    54\n",
       "Filandre     39\n",
       "Agaritte     37\n",
       "Aronthe      37\n",
       "             ..\n",
       "Alexandre     1\n",
       "Nectar        1\n",
       "Vulcain       1\n",
       "Jupiter       1\n",
       "Ciela         1\n",
       "Name: count, Length: 133, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guillaume is the most frequent entity in the document\n",
    "# just because Calyante was not linked to a Wikidata QID\n",
    "# To be dealt with and fixed in the next iteration\n",
    "sss._mentions_df.mention.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
